{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc5518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "                Classifier  Training Accuracy\n",
      "0  Multinomial Naive Bayes           0.589274\n",
      "1      Logistic Regression           0.731398\n",
      "2                 CatBoost           0.684257\n",
      "Submission file saved as: submissions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load training and test data\n",
    "train_file = 'Train.csv'\n",
    "test_file = 'Test.csv'\n",
    "output_file = 'submissions.csv'\n",
    "\n",
    "# Training data does not have a header\n",
    "train_data = pd.read_csv(train_file, header=None, names=['text', 'subreddit'])\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Preprocessing metadata (TF-IDF and N-grams)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Perform dimensionality reduction on TF-IDF using TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "reduced_tfidf = svd.fit_transform(tfidf_features)\n",
    "\n",
    "# Ensure non-negative features for MultinomialNB\n",
    "minmax_scaler = MinMaxScaler()\n",
    "non_negative_tfidf = minmax_scaler.fit_transform(reduced_tfidf)\n",
    "\n",
    "# Normalize TF-IDF features for Logistic Regression and CatBoost\n",
    "tfidf_normalizer = Normalizer(norm='l2')\n",
    "normalized_train_tfidf = tfidf_normalizer.fit_transform(reduced_tfidf)\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "sentence_embeddings = sentence_model.encode(train_data['text'].tolist())\n",
    "\n",
    "# Normalize Sentence Embeddings\n",
    "sentence_normalizer = Normalizer(norm='l2')\n",
    "normalized_train_sentences = sentence_normalizer.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Combine features (Normalized TF-IDF + Normalized Sentence Embeddings)\n",
    "X_combined = np.hstack([\n",
    "    normalized_train_tfidf,              # Normalized TF-IDF features (300 dims)\n",
    "    normalized_train_sentences           # Normalized Sentence Embeddings (84 dims)\n",
    "])\n",
    "\n",
    "# Map labels\n",
    "label_map = {label: idx for idx, label in enumerate(train_data['subreddit'].unique())}\n",
    "y = train_data['subreddit'].map(label_map)\n",
    "\n",
    "# Hyperparameter grids\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.05],\n",
    "    'depth': [4, 6]\n",
    "}\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "grid_search_nb = GridSearchCV(\n",
    "    nb_model, param_grid=param_grid_nb, cv=3, scoring='accuracy', verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search_nb.fit(non_negative_tfidf, y)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "grid_search_lr = GridSearchCV(\n",
    "    lr_model, param_grid=param_grid_lr, cv=3, scoring='accuracy', verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search_lr.fit(X_combined, y)\n",
    "\n",
    "# CatBoost\n",
    "cb_model = CatBoostClassifier(verbose=0)\n",
    "grid_search_cb = GridSearchCV(\n",
    "    cb_model, param_grid=param_grid_cb, cv=3, scoring='accuracy', verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search_cb.fit(X_combined, y)\n",
    "\n",
    "# Save best estimators\n",
    "nb_best_model = grid_search_nb.best_estimator_\n",
    "lr_best_model = grid_search_lr.best_estimator_\n",
    "cb_best_model = grid_search_cb.best_estimator_\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'Classifier': ['Multinomial Naive Bayes', 'Logistic Regression', 'CatBoost'],\n",
    "    'Training Accuracy': [\n",
    "        grid_search_nb.best_score_,\n",
    "        grid_search_lr.best_score_,\n",
    "        grid_search_cb.best_score_\n",
    "    ]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n",
    "\n",
    "# Process test set: TF-IDF\n",
    "test_tfidf_features = tfidf_vectorizer.transform(test_data['body'])\n",
    "test_reduced_tfidf = svd.transform(test_tfidf_features)  # Reduce dimensions to 300\n",
    "test_normalized_tfidf = tfidf_normalizer.transform(test_reduced_tfidf)  # Normalize TF-IDF\n",
    "\n",
    "# Process test set: Sentence Embeddings\n",
    "test_sentence_embeddings = sentence_model.encode(test_data['body'].tolist())\n",
    "test_normalized_sentence_embeddings = sentence_normalizer.transform(test_sentence_embeddings)  # Normalize Sentence Embeddings\n",
    "\n",
    "# Combine test features (TF-IDF + Sentence Embeddings)\n",
    "test_combined = np.hstack([\n",
    "    test_normalized_tfidf,              # Normalized TF-IDF features (300 dims)\n",
    "    test_normalized_sentence_embeddings  # Normalized Sentence Embeddings (84 dims)\n",
    "])\n",
    "\n",
    "# Predict on test set using best CatBoost model\n",
    "test_predictions = cb_best_model.predict(test_combined)\n",
    "\n",
    "# Map predictions back to labels\n",
    "reverse_label_map = {idx: label for label, idx in label_map.items()}\n",
    "\n",
    "# Flatten predictions and map back to labels\n",
    "test_predictions = test_predictions.flatten() if len(test_predictions.shape) > 1 else test_predictions\n",
    "test_data['subreddit'] = [reverse_label_map[int(pred)] for pred in test_predictions]\n",
    "\n",
    "# Create submission file\n",
    "submission = test_data[['id', 'subreddit']]\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"Submission file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cd133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
