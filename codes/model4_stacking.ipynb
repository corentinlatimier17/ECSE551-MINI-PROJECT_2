{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('words')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Define stopwords list\n",
    "specific_stopwords = [\"https\", \"subreddit\", \"www\", \"com\"] ## some specific words for the given dataset\n",
    "stopwords_list = stopwords.words('english') +specific_stopwords + stopwords.words('french') # dataset is both in english and in french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 1399 examples and there are 4 classes\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_training = \"../datasets/Train.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "training_data = pd.read_csv(path_training, delimiter=',')\n",
    "\n",
    "# Set column names explicitly for better readability\n",
    "training_data.columns = ['text', 'subreddit']\n",
    "\n",
    "# Shuffle dataset\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True) \n",
    "\n",
    "# Separate the training data into two series: texts and subreddit labels\n",
    "x_train = training_data['text']          # Contains the Reddit posts or comments\n",
    "y_train = training_data['subreddit'] # Contains the subreddit each post originates from\n",
    "\n",
    "# Get unique subreddit labels\n",
    "unique_labels = np.unique(y_train)   # List of unique subreddits in the dataset\n",
    "\n",
    "n_samples_training = x_train.shape[0]\n",
    "n_classes = unique_labels.shape[0]\n",
    "\n",
    "print(f\"Training dataset has {n_samples_training} examples and there are {n_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has 600 examples\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_test = \"../datasets/Test.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "x_test = pd.read_csv(path_test, delimiter=',')[\"body\"]\n",
    "\n",
    "n_samples_test = x_test.shape[0]\n",
    "print(f\"Test dataset has {n_samples_test} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self, stopwords=None):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stop_words = stopwords\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Tokenize the document and apply lemmatization and filtering\n",
    "        return [\n",
    "            self.wnl.lemmatize(t, pos=\"v\") for t in word_tokenize(doc)\n",
    "            if t.isalpha() and t.lower() not in self.stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = [1 if label == \"Montreal\" else -1 for label in y_train] # for svm training\n",
    "\n",
    "# Define vectorizers\n",
    "vectorizer_svm = TfidfVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=stopwords_list), strip_accents=\"unicode\")\n",
    "vectorizer_bnb = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=stopwords_list), strip_accents=\"unicode\")\n",
    "\n",
    "# Define models\n",
    "svm_model = SVC(kernel='rbf', probability=True, gamma='scale', C=1)\n",
    "bnb_model = BernoulliNB()\n",
    "\n",
    "# Define feature selectors\n",
    "selector_bnb = SelectKBest(mutual_info_classif, k=2850)\n",
    "selector_svm = SelectKBest(mutual_info_classif, k=3000)\n",
    "\n",
    "# Define scaler \n",
    "scaler_svm = StandardScaler()\n",
    "\n",
    "# Preprocess data before cross-validation\n",
    "X_train_bnb = vectorizer_bnb.fit_transform(x_train)\n",
    "X_train_svm = vectorizer_svm.fit_transform(x_train)\n",
    "\n",
    "# Apply feature selection\n",
    "X_train_bnb_selected = selector_bnb.fit_transform(X_train_bnb, y_train)\n",
    "X_train_svm_selected = selector_svm.fit_transform(X_train_svm, y_binary)\n",
    "\n",
    "# Scale the SVM features\n",
    "X_train_svm_scaled = scaler_svm.fit_transform(np.asarray(X_train_svm_selected.todense()))\n",
    "\n",
    "# Prepare KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "training_accuracies = []\n",
    "class_accuracies = {class_name: [] for class_name in set(y_train)}  # To store accuracy for each class\n",
    "\n",
    "fold = 0\n",
    "\n",
    "# 10-Fold Cross-Validation\n",
    "time_start = time.time()\n",
    "for train_index, val_index in kf.split(X_train_svm_scaled):\n",
    "    fold += 1\n",
    "    # Split data into training and validation sets\n",
    "    X_train_fold_svm, X_val_fold_svm = X_train_svm_scaled[train_index], X_train_svm_scaled[val_index]\n",
    "    X_train_fold_bnb, X_val_fold_bnb = X_train_bnb_selected[train_index], X_train_bnb_selected[val_index]\n",
    "    y_train_bnb_fold, y_val_bnb_fold = np.array(y_train)[train_index], np.array(y_train)[val_index]\n",
    "    y_train_svm_fold, y_val_svm_fold = np.array(y_binary)[train_index], np.array(y_binary)[val_index]\n",
    "\n",
    "    # Train the models\n",
    "    svm_model.fit(X_train_fold_svm, y_train_svm_fold)\n",
    "    bnb_model.fit(X_train_fold_bnb, y_train_bnb_fold)\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    svm_predictions = svm_model.predict(X_val_fold_svm)\n",
    "    bnb_predictions = bnb_model.predict(X_val_fold_bnb)\n",
    "    # Initialize an empty list for final predictions\n",
    "    final_predictions = []\n",
    "   # Vectorized version of combining predictions\n",
    "    final_predictions = np.where(svm_predictions == 1, \"Montreal\", bnb_predictions)\n",
    "\n",
    "    # Get predictions from both models for training data\n",
    "    svm_predictions = svm_model.predict(X_train_fold_svm)\n",
    "    bnb_predictions = bnb_model.predict(X_train_fold_bnb)\n",
    "    # Initialize an empty list for final predictions\n",
    "    final_predictions_training = []\n",
    "   # Vectorized version of combining predictions\n",
    "    final_predictions_training = np.where(svm_predictions == 1, \"Montreal\", bnb_predictions)\n",
    "\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "    accuracy = accuracy_score(y_val_bnb_fold, final_predictions)  # Use y_val_bnb_fold as the correct target variable\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    training_accuracy  = accuracy_score(y_train_bnb_fold, final_predictions_training)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val_bnb_fold, final_predictions))\n",
    "    class_accuracy = classification_report(y_val_bnb_fold, final_predictions, output_dict=True)\n",
    "\n",
    "    for label, metrics in class_accuracy.items():\n",
    "        if label != 'accuracy' and label!=\"macro avg\" and label!= \"weighted avg\": \n",
    "            class_accuracies[label].append(metrics['precision'])\n",
    "\n",
    "    print(f\"Validation accuracy for fold {fold}: {accuracy:.4f}\")\n",
    "    print(f\"Training accuracy for fold {fold}: {training_accuracy:.4f}\\n\")\n",
    "time_end = time.time()\n",
    "\n",
    "# Calculate the mean accuracy across all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy across 10 folds: {mean_accuracy:.4f}\")\n",
    "\n",
    "mean_training_accuracy = np.mean(training_accuracies)\n",
    "print(f\"Mean Accuracy across 10 folds: {mean_training_accuracy:.4f}\")\n",
    "\n",
    "# Average accuracy for each class\n",
    "print(\"\\nAverage Accuracy per Class:\")\n",
    "for label, accuracies in class_accuracies.items():\n",
    "    avg_class_accuracy = np.mean(accuracies)\n",
    "    print(f\"Class {label}: {avg_class_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Computing time : {time_end-time_start} (s)\")\n",
    "\n",
    "# Fitting the models with the whole dataset\n",
    "svm_model.fit(X_train_svm_scaled, y_binary)\n",
    "bnb_model.fit(X_train_bnb_selected, y_train)\n",
    "\n",
    "# Preprocess x_test\n",
    "x_test_bnb = vectorizer_bnb.transform(x_test)\n",
    "x_test_svm = vectorizer_svm.transform(x_test)\n",
    "\n",
    "x_test_bnb_selected = selector_bnb.transform(x_test_bnb)\n",
    "x_test_svm_selected = selector_svm.transform(x_test_svm)\n",
    "\n",
    "x_test_svm_scaled = scaler_svm.transform(np.asarray(x_test_svm_selected.todense()))\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = svm_model.predict(x_test_svm_scaled)\n",
    "bnb_predictions = bnb_model.predict(x_test_bnb_selected)\n",
    "final_predictions = []\n",
    "final_predictions = np.where(svm_predictions == 1, \"Montreal\", bnb_predictions)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'id': range(len(final_predictions)),\n",
    "    'subreddit': final_predictions\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"../output/stacking.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
