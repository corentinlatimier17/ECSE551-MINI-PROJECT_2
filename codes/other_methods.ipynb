{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as ss\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('words')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Define stopwords list\n",
    "specific_stopwords = [\"https\", \"subreddit\", \"www\", \"com\"] ## some specific words for the given dataset\n",
    "stopwords_list = stopwords.words('english') +specific_stopwords + stopwords.words('french') # dataset is both in english and in french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 1399 examples and there are 4 classes\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_training = \"../datasets/Train.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "training_data = pd.read_csv(path_training, delimiter=',')\n",
    "\n",
    "# Set column names explicitly for better readability\n",
    "training_data.columns = ['text', 'subreddit']\n",
    "\n",
    "# Shuffle dataset\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True) \n",
    "\n",
    "# Separate the training data into two series: texts and subreddit labels\n",
    "x_train = training_data['text']          # Contains the Reddit posts or comments\n",
    "y_train = training_data['subreddit'] # Contains the subreddit each post originates from\n",
    "\n",
    "# Get unique subreddit labels\n",
    "unique_labels = np.unique(y_train)   # List of unique subreddits in the dataset\n",
    "\n",
    "n_samples_training = x_train.shape[0]\n",
    "n_classes = unique_labels.shape[0]\n",
    "\n",
    "print(f\"Training dataset has {n_samples_training} examples and there are {n_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has 600 examples\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_test = \"../datasets/Test.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "x_test = pd.read_csv(path_test, delimiter=',')[\"body\"]\n",
    "\n",
    "n_samples_test = x_test.shape[0]\n",
    "print(f\"Test dataset has {n_samples_test} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self, stopwords=None):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stop_words = stopwords\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Tokenize the document and apply lemmatization and filtering\n",
    "        return [\n",
    "            self.wnl.lemmatize(t, pos=\"v\") for t in word_tokenize(doc)\n",
    "            if t.isalpha() and t.lower() not in self.stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC - Two classifiers method #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "if False:\n",
    "    #################################################################### Classifier 1 : Montreal or Not ######################################################################\n",
    "    # Best parameters found : max_features = 1000, svm_C = 10, kernel=\"rbf\", svm_gamma = 'scale'\n",
    "\n",
    "    # Convert y_train to binary labels for the first classifier\n",
    "    y_binary = np.array([1 if label == \"Montreal\" else 0 for label in y_train])\n",
    "\n",
    "    TfidfVectorizer1 = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=1000,\n",
    "        tokenizer=LemmaTokenizer(stopwords_list)\n",
    "    )\n",
    "\n",
    "    x_tfidf1 = TfidfVectorizer1.fit_transform(x_train)\n",
    "    scaler1 = StandardScaler()\n",
    "    x_scaled_1 = scaler1.fit_transform(np.asarray(x_tfidf1.todense()))\n",
    "\n",
    "    binary_svm = SVC(C=10.0, kernel=\"rbf\", gamma=\"scale\")\n",
    "\n",
    "    # 10 Fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_scaled_1):\n",
    "        X_train_fold, X_val_fold = x_scaled_1[train_index], x_scaled_1[val_index]\n",
    "        y_fold_train, y_fold_val = y_binary[train_index], y_binary[val_index]\n",
    "        \n",
    "        # Train best binary SVM classifier\n",
    "        binary_svm.fit(X_train_fold, y_fold_train)\n",
    "        \n",
    "        # Predict and evaluate on validation set\n",
    "        y_pred = binary_svm.predict(X_val_fold)\n",
    "        accuracy = accuracy_score(y_fold_val, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Display results for each fold\n",
    "        print(f\"Accuracy for fold: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\\n\", classification_report(y_fold_val, y_pred))\n",
    "\n",
    "    # Calculate and display mean accuracy\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Mean Accuracy across 10 folds for best binary classifier: {mean_accuracy:.4f}\")\n",
    "\n",
    "    #################################################################### Classifier 2 : Other subreddits ######################################################################\n",
    "\n",
    "    # Best parameters for multi-class SVM: {'svm__C': 0.1, 'svm__gamma': 'scale', 'svm__kernel': 'linear', 'tfidf__max_features': 4000}\n",
    "\n",
    "    # Filter for \"Not Montreal\" entries for the second classifier\n",
    "    x_not_montreal = np.array(x_train)[y_binary == 0]\n",
    "    y_not_montreal = np.array(y_train)[y_binary == 0]\n",
    "\n",
    "    TfidfVectorizer2 = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=4000,\n",
    "        stop_words=stopwords_list\n",
    "    )\n",
    "    x_tfidf2 = TfidfVectorizer2.fit_transform(x_not_montreal)\n",
    "    scaler2 = StandardScaler()\n",
    "    x_scaled_2 = scaler2.fit_transform(np.asarray(x_tfidf2.todense()))\n",
    "\n",
    "    multi_class_svm = SVC(C=0.1, kernel=\"linear\", gamma=\"scale\")\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_scaled_2):\n",
    "        X_train_fold, X_val_fold = x_scaled_2[train_index], x_scaled_2[val_index]\n",
    "        y_fold_train, y_fold_val = y_not_montreal[train_index], y_not_montreal[val_index]\n",
    "        \n",
    "        # Train best multi-class SVM on non-Montreal data\n",
    "        multi_class_svm.fit(X_train_fold, y_fold_train)\n",
    "        \n",
    "        # Predict and evaluate on validation set\n",
    "        y_pred = multi_class_svm.predict(X_val_fold)\n",
    "        accuracy = accuracy_score(y_fold_val, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Display results for each fold\n",
    "        print(f\"Accuracy for fold: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\\n\", classification_report(y_fold_val, y_pred))\n",
    "\n",
    "    # Calculate and display mean accuracy for the multi-class classifier\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Mean Accuracy across 10 folds for best multi-class classifier: {mean_accuracy:.4f}\")\n",
    "\n",
    "    ################################################################################ Fit the two classifiers with the whole dataset #########################################\n",
    "    binary_svm.fit(x_scaled_1, y_binary)\n",
    "    multi_class_svm.fit(x_scaled_2, y_not_montreal)\n",
    "\n",
    "    ######################################################################### Save predictions in a csv file ##############################################################\n",
    "    def predict_subreddits_SVC(x_test, binary_model, multi_class_model, vectorizer_binary, vectorizer_multiclass, scaler_binary, scaler_multiclass):\n",
    "        \"\"\"\n",
    "        Predicts the location for a given array of texts using a binary and multi-class model.\n",
    "        Saves predictions in a CSV file with 'id' and 'subreddit' columns.\n",
    "\n",
    "        Parameters:\n",
    "        - x_test (array-like): Array of text inputs to predict.\n",
    "        - binary_model (sklearn model): Fitted binary SVM model.\n",
    "        - multi_class_model (sklearn model): Fitted multi-class SVM model.\n",
    "        - vectorizer_binary (TfidfVectorizer): Fitted TF-IDF vectorizer for binary model.\n",
    "        - vectorizer_multiclass (TfidfVectorizer): Fitted TF-IDF vectorizer for multi-class model.\n",
    "        - scaler_binary (StandardScaler): Fitted scaler for binary model.\n",
    "        - scaler_multiclass (StandardScaler): Fitted scaler for multi-class model.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Prediction results with 'id' and 'subreddit' columns.\n",
    "        \"\"\"\n",
    "        # Step 1: Transform texts using the vectorizer and scaler for binary model\n",
    "        x_test_tfidf1 = vectorizer_binary.transform(x_test)\n",
    "        x_test_scaled_1 = scaler_binary.transform(np.asarray(x_test_tfidf1.todense()))\n",
    "        \n",
    "        # Step 2: Predict with the binary model to identify \"Montreal\" vs \"Not Montreal\"\n",
    "        binary_predictions = binary_model.predict(x_test_scaled_1)\n",
    "        \n",
    "        # Step 3: Prepare to apply multi-class model only to \"Not Montreal\" entries\n",
    "        not_montreal_mask = (binary_predictions == 0)  # Mask for \"Not Montreal\" entries\n",
    "        x_test_not_mtl = x_test[not_montreal_mask]\n",
    "\n",
    "        # Step 4: Transform \"Not Montreal\" texts using multi-class vectorizer\n",
    "        x_test_tfidf2 = vectorizer_multiclass.transform(x_test_not_mtl)\n",
    "        x_test_scaled_2 = scaler_multiclass.transform(np.asarray(x_test_tfidf2.todense()))\n",
    "        \n",
    "        # Apply multi-class model only to the \"Not Montreal\" subset\n",
    "        multi_class_predictions = multi_class_model.predict(x_test_scaled_2)\n",
    "        \n",
    "        # Step 5: Combine predictions for both \"Montreal\" and multi-class predictions\n",
    "        predictions = []\n",
    "        \n",
    "        # Add \"Montreal\" predictions\n",
    "        for pred in binary_predictions:\n",
    "            if pred == 1:\n",
    "                predictions.append(\"Montreal\")\n",
    "            else:\n",
    "                predictions.append(None)  # Placeholder for \"Not Montreal\" predictions\n",
    "                \n",
    "        # Replace None with multi-class predictions for \"Not Montreal\" entries\n",
    "        not_montreal_indices = [i for i, pred in enumerate(binary_predictions) if pred == 0]\n",
    "        for idx, multi_pred in zip(not_montreal_indices, multi_class_predictions):\n",
    "            predictions[idx] = multi_pred\n",
    "        \n",
    "        # Step 6: Create DataFrame for results\n",
    "        results_df = pd.DataFrame({\n",
    "            'id': range(len(x_test)),\n",
    "            'subreddit': predictions\n",
    "        })\n",
    "        \n",
    "        # Step 7: Save to CSV\n",
    "        results_df.to_csv(\"../output/submissions_two_layer_svm.csv\", index=False)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "\n",
    "    predict_subreddits_SVC(x_test, binary_svm, multi_class_svm, TfidfVectorizer1, TfidfVectorizer2, scaler1, scaler2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clatimie/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build the network\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + tf.exp(-z))\n",
    "\n",
    "def classifier_model(layer_size, num_layers, activation, input_dim, dropout_rate=0.7):\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # Add first layer\n",
    "    network.add(layers.Dense(layer_size, \n",
    "                             input_dim=input_dim, \n",
    "                             activation=activation, \n",
    "                             kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                             kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        network.add(layers.Dense(layer_size, \n",
    "                                 activation=activation, \n",
    "                                 kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "        network.add(BatchNormalization())\n",
    "        network.add(layers.Dropout(dropout_rate))\n",
    "    network.add(layers.Dense(4, activation='softmax')) # Add the output layer\n",
    "\n",
    "    # Compile the network\n",
    "    network.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return network\n",
    "\n",
    "model_1 = classifier_model(\n",
    "    layer_size=200,\n",
    "    num_layers=5, \n",
    "    input_dim=3000,\n",
    "    activation=sigmoid,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.2620 - loss: 3.5082\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4216 - loss: 2.7844\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5576 - loss: 2.2899\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7703 - loss: 1.6791\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8324 - loss: 1.4269\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8464 - loss: 1.3424 \n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8226 - loss: 1.3617\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8704 - loss: 1.1832\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9059 - loss: 1.0430\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8976 - loss: 1.0112\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6179 - loss: 1.6662\n",
      "Validation Accuracy for this fold: 0.6429\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8475 - loss: 1.1462\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9053 - loss: 0.9289\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9075 - loss: 0.8853\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9089 - loss: 0.8349\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9171 - loss: 0.8286\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9039 - loss: 0.8434 \n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9192 - loss: 0.7529\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9365 - loss: 0.7335\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9374 - loss: 0.6834\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9246 - loss: 0.7376\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6980 - loss: 1.4072 \n",
      "Validation Accuracy for this fold: 0.6929\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8962 - loss: 0.8199\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9182 - loss: 0.7293\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9468 - loss: 0.6103\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9372 - loss: 0.6403\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9159 - loss: 0.6757\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9327 - loss: 0.6564\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9288 - loss: 0.6459\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9390 - loss: 0.6029\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9322 - loss: 0.6313\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9362 - loss: 0.6555\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7780 - loss: 1.0763 \n",
      "Validation Accuracy for this fold: 0.7714\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9263 - loss: 0.6820\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9239 - loss: 0.6787\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9444 - loss: 0.6088\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9435 - loss: 0.6094\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9619 - loss: 0.5289\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9380 - loss: 0.6320\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9284 - loss: 0.6179\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9301 - loss: 0.6366\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9525 - loss: 0.5746\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9622 - loss: 0.5558\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7506 - loss: 1.2119 \n",
      "Validation Accuracy for this fold: 0.7714\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9152 - loss: 0.7050\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9609 - loss: 0.5724\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9272 - loss: 0.6696\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9519 - loss: 0.5694\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9478 - loss: 0.6230\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9247 - loss: 0.6876\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9421 - loss: 0.6105\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9521 - loss: 0.5734\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9430 - loss: 0.6118\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9582 - loss: 0.5698\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8855 - loss: 0.7745 \n",
      "Validation Accuracy for this fold: 0.8571\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9461 - loss: 0.6005\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9361 - loss: 0.6259\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9412 - loss: 0.6323\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9433 - loss: 0.6059\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9414 - loss: 0.6247\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9519 - loss: 0.5850\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9304 - loss: 0.6625\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9326 - loss: 0.6092\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9616 - loss: 0.5668\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9524 - loss: 0.5433\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8348 - loss: 0.9121 \n",
      "Validation Accuracy for this fold: 0.8286\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9236 - loss: 0.6340\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9300 - loss: 0.6796\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9573 - loss: 0.6017\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9483 - loss: 0.5958\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9330 - loss: 0.6855\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9408 - loss: 0.6332\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9442 - loss: 0.6347\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9574 - loss: 0.5952\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9471 - loss: 0.5734\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9590 - loss: 0.5535\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9115 - loss: 0.6587 \n",
      "Validation Accuracy for this fold: 0.9286\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9540 - loss: 0.5566\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9543 - loss: 0.5408\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9216 - loss: 0.6583\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9444 - loss: 0.5848\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9431 - loss: 0.6185\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9393 - loss: 0.6454\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9480 - loss: 0.6031\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9525 - loss: 0.5966\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9572 - loss: 0.5613\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9542 - loss: 0.5669\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9337 - loss: 0.6981 \n",
      "Validation Accuracy for this fold: 0.9286\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9436 - loss: 0.5864\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9200 - loss: 0.7025\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9662 - loss: 0.5736\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9567 - loss: 0.5885\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9528 - loss: 0.5319\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9439 - loss: 0.6305\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9521 - loss: 0.5871\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9445 - loss: 0.6307\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9438 - loss: 0.6190\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9503 - loss: 0.5822\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8534 - loss: 0.9265 \n",
      "Validation Accuracy for this fold: 0.8714\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9290 - loss: 0.6740\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9426 - loss: 0.6425\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9440 - loss: 0.6193\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9450 - loss: 0.6020\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9511 - loss: 0.5952\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9470 - loss: 0.6082\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9584 - loss: 0.5595\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9352 - loss: 0.6262\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9549 - loss: 0.5574\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9463 - loss: 0.6057\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9087 - loss: 0.7707\n",
      "Validation Accuracy for this fold: 0.9137\n",
      "Mean Validation Accuracy: 0.8207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming y_train contains string labels like 'Montreal', 'Toronto', etc.\n",
    "# Step 1: Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Converts string labels to integers\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "y_train_cat = to_categorical(y_train_encoded, num_classes=4)  # Assuming 4 classes\n",
    "\n",
    "# Prepare the TfidfVectorizer and apply it to the training set\n",
    "TfidfVectorizer_nn = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=3000,\n",
    "    tokenizer=LemmaTokenizer(stopwords=stopwords_list)  # Your LemmaTokenizer\n",
    ")\n",
    "\n",
    "# Transform the text data to the TF-IDF matrix\n",
    "x_tfidf_train_nn = TfidfVectorizer_nn.fit_transform(x_train)  # Assuming x_train is your training text\n",
    "scaler_nn = StandardScaler()\n",
    "x_scaled_train_nn = scaler_nn.fit_transform(np.asarray(x_tfidf_train_nn.todense()))  # Scale the data\n",
    "\n",
    "# Proceed with k-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []  # To store accuracies for each fold\n",
    "\n",
    "for train_index, val_index in kf.split(x_scaled_train_nn):\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = x_scaled_train_nn[train_index], x_scaled_train_nn[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_cat[train_index], y_train_cat[val_index]\n",
    "    \n",
    "\n",
    "    # Train the model on the training fold\n",
    "    model_1.fit(X_train_fold, y_train_fold, epochs=20, batch_size=32, verbose=1)  # Adjust epochs and batch size\n",
    "\n",
    "    # Evaluate the model on the validation fold\n",
    "    val_loss, val_accuracy = model_1.evaluate(X_val_fold, y_val_fold)\n",
    "    accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Validation Accuracy for this fold: {val_accuracy:.4f}\")\n",
    "\n",
    "# Print mean accuracy across all folds\n",
    "print(f\"Mean Validation Accuracy: {np.mean(accuracies):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
