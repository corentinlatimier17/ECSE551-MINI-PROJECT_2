{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Subreddit prediction</u> #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description of the project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Project overview </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project aims to develop machine learning models for **analyzing Reddit text** to determine the origin subreddit of a given post or comment. Reddit, a popular social media platform, is organized into a variety of thematic communities known as *subreddits*, where users share content and engage in discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color: #FF9800;\">Objective </span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective is to build a model that can **predict the subreddit** of a Reddit post or comment. Given a text entry from Reddit, the model will identify which of the following subreddits it originally came from:\n",
    "\n",
    "- **Toronto**\n",
    "- **Brussels**\n",
    "- **London**\n",
    "- **Montreal**\n",
    "\n",
    "<b>This defines a multiclass classification problem</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Approach</span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project consists of two main parts:\n",
    "\n",
    "1. **Implement a Bernoulli Naïve Bayes Classifier from Scratch**  \n",
    "   First, a Bernoulli Naïve Bayes classifier will be developed from the ground up, without relying on external libraries for the core algorithm. This implementation will provide a deeper understanding of how the Bernoulli Naïve Bayes method works and how it can be applied to text classification.\n",
    "\n",
    "2. **Utilize a Classifier from Scikit-Learn**  \n",
    "   In the second part, a pre-built classifier from the `scikit-learn` library will be used to perform the same task. This comparison will allow us to evaluate the effectiveness of our custom implementation against a widely used, optimized machine learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset and modules ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Module importation </span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Ensure required NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Load training dataset</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 1399 examples and 4 classes\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_training = \"../datasets/Train.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "training_data = pd.read_csv(path_training, delimiter=',')\n",
    "\n",
    "# Set column names explicitly for better readability\n",
    "training_data.columns = ['text', 'subreddit']\n",
    "\n",
    "# Separate the training data into two series: texts and subreddit labels\n",
    "texts_train = training_data['text']          # Contains the Reddit posts or comments\n",
    "subreddits_train = training_data['subreddit'] # Contains the subreddit each post originates from\n",
    "\n",
    "# Get unique subreddit labels\n",
    "unique_labels = np.unique(subreddits_train)   # List of unique subreddits in the dataset\n",
    "\n",
    "n_samples = texts_train.shape[0]\n",
    "n_classes = unique_labels.shape[0]\n",
    "\n",
    "print(f\"Dataset has {n_samples} examples and {n_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization of the texts ##\n",
    "\n",
    "To utilize the texts in machine learning models, it is essential to convert them into a vectorized format. There are several methods available for encoding texts as vectors.\n",
    "\n",
    "1. **Binary Representation of Words**  \n",
    "   One approach is to employ a binary representation of the words. This method indicates the presence or absence of each word in the text using binary values (1 or 0).\n",
    "\n",
    "2. **Removal of Stop Words**  \n",
    "   Additionally, it is important to consider the removal of stop words—common words such as \"and,\" \"the,\" or \"is\" that may not carry significant meaning in the context of the analysis. By eliminating these words, we can enhance the quality of our feature set.\n",
    "\n",
    "3. **TF-IDF (Term Frequency-Inverse Document Frequency)**  \n",
    "   Another effective technique for vectorization is the use of TF-IDF. This method not only accounts for the frequency of words in the text but also adjusts for their importance across the entire corpus. By selecting features based on TF-IDF scores, we can focus on the most relevant words for our machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized dataset(WITHOUT stop words consideration) has 1399 examples and 13690 features\n",
      "Binary vectorized dataset(WITH stop words consideration) has 1399 examples and 13461 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized dataset (WITH stop words consideration and STEMMING) has 1399 examples and 8635 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized dataset (WITH stop words consideration and STEMMING and TFIDF based token selection) has 1399 examples and 5231 features\n"
     ]
    }
   ],
   "source": [
    "# Approach 1 :  Binary representation of words (present [1] or absent [0]) -> no stop words considered and no words selection\n",
    "BinaryVectorizer = CountVectorizer(binary=True)\n",
    "x_train_1 = BinaryVectorizer.fit_transform(texts_train)\n",
    "n_features_1 = x_train_1.shape[1]\n",
    "print(f\"Binary vectorized dataset(WITHOUT stop words consideration) has {n_samples} examples and {n_features_1} features\")\n",
    "\n",
    "\n",
    "stopwords_list = stopwords.words('english') + stopwords.words('french') # document is both in english and french\n",
    "\n",
    "# Approach 2 : Binary representation of words (present [1] or absent [0]) with stop words considered\n",
    "BinaryVectorizerStopWords = CountVectorizer(\n",
    "    binary=True, \n",
    "    stop_words=stopwords_list)\n",
    "\n",
    "x_train_2 = BinaryVectorizerStopWords.fit_transform(texts_train)\n",
    "n_features_2 = x_train_2.shape[1]\n",
    "print(f\"Binary vectorized dataset(WITH stop words consideration) has {n_samples} examples and {n_features_2} features\")\n",
    "\n",
    "# Approach 3: Binary representation of words (present [1] or absent [0]) with stop words considered and stemming\n",
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        # Initialize the Porter Stemmer\n",
    "        self.wnl = nltk.stem.PorterStemmer()\n",
    "        self.stop_words = stopwords_list  \n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Tokenize the document and stem each token, filtering out non-alpha and stop words\n",
    "        return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha() and t not in self.stop_words]\n",
    "\n",
    "# Set up the CountVectorizer with binary representation, stop words, and stemming\n",
    "BinaryVectorizerStopWordsandStemming = CountVectorizer(\n",
    "    binary=True,\n",
    "    tokenizer=StemTokenizer(),\n",
    "    stop_words=stopwords_list\n",
    ")\n",
    "x_train_3 = BinaryVectorizerStopWordsandStemming.fit_transform(texts_train)\n",
    "n_features_3 = x_train_3.shape[1]\n",
    "print(f\"Binary vectorized dataset (WITH stop words consideration and STEMMING) has {n_samples} examples and {n_features_3} features\")\n",
    "\n",
    "# Approach 4: Binary representation of words (present [1] or absent [0]) with stop words considered and stemming and selection based on TD-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords_list, use_idf=True, smooth_idf=True, tokenizer=StemTokenizer())\n",
    "\n",
    "# Fit and transform the training texts\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(texts_train)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense format \n",
    "dense_tfidf = x_train_tfidf.todense()\n",
    "tfidf_df = pd.DataFrame(dense_tfidf, columns=feature_names)\n",
    "\n",
    "# select words based on a threshold\n",
    "threshold = 0.2  # Hyperparameter\n",
    "important_tokens = tfidf_df.columns[(tfidf_df > threshold).any(axis=0)]\n",
    "\n",
    "# Set up the CountVectorizer with binary representation, stop words, stemming and token selection based on TF-IDF\n",
    "BinaryVectorizerStopWordsandStemmingandTFIDF = CountVectorizer(\n",
    "    binary=True,\n",
    "    tokenizer=StemTokenizer(),\n",
    "    stop_words=stopwords_list,\n",
    "    vocabulary=important_tokens\n",
    ")\n",
    "x_train_4 = BinaryVectorizerStopWordsandStemmingandTFIDF.fit_transform(texts_train)\n",
    "n_features_4 = x_train_4.shape[1]\n",
    "print(f\"Binary vectorized dataset (WITH stop words consideration and STEMMING and TFIDF based token selection) has {n_samples} examples and {n_features_4} features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
