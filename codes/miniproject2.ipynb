{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Subreddit prediction</u> #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description of the project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Project overview </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project aims to develop machine learning models for **analyzing Reddit text** to determine the origin subreddit of a given post or comment. Reddit, a popular social media platform, is organized into a variety of thematic communities known as *subreddits*, where users share content and engage in discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color: #FF9800;\">Objective </span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective is to build a model that can **predict the subreddit** of a Reddit post or comment. Given a text entry from Reddit, the model will identify which of the following subreddits it originally came from:\n",
    "\n",
    "- **Toronto**\n",
    "- **Brussels**\n",
    "- **London**\n",
    "- **Montreal**\n",
    "\n",
    "<b>This defines a multiclass classification problem</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Approach</span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project consists of two main parts:\n",
    "\n",
    "1. **Implement a Bernoulli Naïve Bayes Classifier from Scratch**  \n",
    "   First, a Bernoulli Naïve Bayes classifier will be developed from the ground up, without relying on external libraries for the core algorithm. This implementation will provide a deeper understanding of how the Bernoulli Naïve Bayes method works and how it can be applied to text classification.\n",
    "\n",
    "2. **Utilize a Classifier from Scikit-Learn**  \n",
    "   In the second part, a pre-built classifier from the `scikit-learn` library will be used to perform the same task. This comparison will allow us to evaluate the effectiveness of our custom implementation against a widely used, optimized machine learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset and modules ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Module importation </span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/clatimie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Ensure required NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Load training dataset</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 1399 examples and there are 4 classes\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_training = \"../datasets/Train.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "training_data = pd.read_csv(path_training, delimiter=',')\n",
    "\n",
    "# Set column names explicitly for better readability\n",
    "training_data.columns = ['text', 'subreddit']\n",
    "\n",
    "# Separate the training data into two series: texts and subreddit labels\n",
    "texts_train = training_data['text']          # Contains the Reddit posts or comments\n",
    "subreddits_train = training_data['subreddit'] # Contains the subreddit each post originates from\n",
    "\n",
    "# Get unique subreddit labels\n",
    "unique_labels = np.unique(subreddits_train)   # List of unique subreddits in the dataset\n",
    "\n",
    "n_samples_training = texts_train.shape[0]\n",
    "n_classes = unique_labels.shape[0]\n",
    "\n",
    "print(f\"Training dataset has {n_samples_training} examples and there are {n_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Load test dataset</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has 600 examples\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_test = \"../datasets/Test.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "texts_test = pd.read_csv(path_test, delimiter=',')[\"body\"]\n",
    "\n",
    "n_samples_test = texts_test.shape[0]\n",
    "print(f\"Test dataset has {n_samples_test} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization of the texts ##\n",
    "\n",
    "To utilize the texts in machine learning models, it is essential to convert them into a vectorized format. There are several methods available for encoding texts as vectors.\n",
    "\n",
    "1. **Binary Representation of Words**  \n",
    "   One approach is to employ a binary representation of the words. This method indicates the presence or absence of each word in the text using binary values (1 or 0).\n",
    "\n",
    "2. **Removal of Stop Words**  \n",
    "   Additionally, it is important to consider the removal of stop words—common words such as \"and,\" \"the,\" or \"is\" that may not carry significant meaning in the context of the analysis. By eliminating these words, we can enhance the quality of our feature set.\n",
    "\n",
    "3. **TF-IDF (Term Frequency-Inverse Document Frequency)**  \n",
    "   Another effective technique for vectorization is the use of TF-IDF. This method not only accounts for the frequency of words in the text but also adjusts for their importance across the entire corpus. By selecting features based on TF-IDF scores, we can focus on the most relevant words for our machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTokenizer:\n",
    "            def __init__(self, stopwords_list):\n",
    "                # Initialize the Porter Stemmer\n",
    "                self.wnl = nltk.stem.PorterStemmer()\n",
    "                self.stop_words = stopwords_list\n",
    "\n",
    "            def __call__(self, doc):\n",
    "                # Tokenize the document and stem each token, filtering out non-alpha and stop words\n",
    "                return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha() and t not in self.stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self, method,isTraining,threshold=None, stopwords_list=None):\n",
    "        self.method = method\n",
    "        self.isTraining = isTraining\n",
    "        self.vectorizer = None\n",
    "        self.threshold = threshold \n",
    "        self.stopwords = stopwords_list\n",
    "    \n",
    "    def change_mode(self):\n",
    "        if self.isTraining==True:\n",
    "            self.isTraining=False\n",
    "        else:\n",
    "            self.isTraining=True\n",
    "\n",
    "    def vectorize(self, texts):\n",
    "        if self.method == \"binary\":\n",
    "            if self.isTraining : \n",
    "                return self.BinaryVectorizer(texts)\n",
    "            else:\n",
    "                # Just transform the data if in testing mode\n",
    "                x_test= self.vectorizer.transform(texts)\n",
    "                print(f\"Binary vectorized test dataset has {x_test.shape[0]} samples and {x_test.shape[1]} features\")\n",
    "                return x_test\n",
    "        elif self.method == \"binaryANDstopwords\":\n",
    "            if self.isTraining : \n",
    "                return self.BinaryVectorizerANDstopwords(texts)\n",
    "            else:\n",
    "                # Just transform the data if in testing mode\n",
    "                x_test= self.vectorizer.transform(texts)\n",
    "                print(f\"Binary vectorized test dataset (WITH stop words consideration) has {x_test.shape[0]} samples and {x_test.shape[1]} features\")\n",
    "                return x_test\n",
    "        elif self.method == \"binaryANDstopwordsANDstemming\":\n",
    "            if self.isTraining:\n",
    "                return self.BinaryVectorizerANDstopwordsANDstemming(texts)\n",
    "            else:\n",
    "                # Just transform the data if in testing mode\n",
    "                x_test= self.vectorizer.transform(texts)\n",
    "                print(f\"Binary vectorized test dataset (WITH stop words consideration AND stemming) has {x_test.shape[0]} samples and {x_test.shape[1]} features\")\n",
    "                return x_test\n",
    "        elif self.method == \"binaryANDstopwordsANDstemmingANDtfidf\":\n",
    "            if self.isTraining:\n",
    "                return self.BinaryVectorizerANDstopwordsANDstemmingANDtfidf(texts)\n",
    "            else:\n",
    "                # Just transform the data if in testing mode\n",
    "                x_test= self.vectorizer.transform(texts)\n",
    "                print(f\"Binary vectorized test dataset (WITH stop words consideration and STEMMING and TFIDF based token selection [thresold = {self.threshold}]) has {x_test.shape[0]} examples and {x_test.shape[1]} features\")\n",
    "                return x_test\n",
    "    \n",
    "    def BinaryVectorizer(self, texts):\n",
    "        self.vectorizer = CountVectorizer(binary=True)\n",
    "        x_train = self.vectorizer.fit_transform(texts)\n",
    "        n_samples = x_train.shape[0]  # Number of samples\n",
    "        print(f\"Binary vectorized training dataset has {n_samples} examples and {x_train.shape[1]} features\")\n",
    "        return x_train\n",
    "    \n",
    "    def BinaryVectorizerANDstopwords(self, texts):\n",
    "        self.vectorizer = CountVectorizer(binary=True, stop_words=self.stopwords)\n",
    "        x_train = self.vectorizer.fit_transform(texts)\n",
    "        n_samples = x_train.shape[0]  # Number of samples\n",
    "        print(f\"Binary vectorized training dataset (WITH stop words consideration) has {n_samples} examples and {x_train.shape[1]} features\")\n",
    "        return x_train\n",
    "    \n",
    "    def BinaryVectorizerANDstopwordsANDstemming(self, texts):\n",
    "        class StemTokenizer:\n",
    "            def __init__(self, stop_words):\n",
    "                # Initialize the Porter Stemmer\n",
    "                self.wnl = nltk.stem.PorterStemmer()\n",
    "                self.stop_words = stop_words  \n",
    "\n",
    "            def __call__(self, doc):\n",
    "                # Tokenize the document and stem each token, filtering out non-alpha and stop words\n",
    "                return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha() and t not in self.stop_words]\n",
    "\n",
    "        # Set up the CountVectorizer with binary representation, stop words, and stemming\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            binary=True,\n",
    "            tokenizer=StemTokenizer(self.stopwords),\n",
    "            stop_words=self.stopwords\n",
    "        )\n",
    "        x_train = self.vectorizer.fit_transform(texts)\n",
    "        n_samples = x_train.shape[0]  # Number of samples\n",
    "        print(f\"Binary vectorized training dataset (WITH stop words consideration and STEMMING) has {n_samples} examples and {x_train.shape[1]} features\")\n",
    "        return x_train\n",
    "\n",
    "    def BinaryVectorizerANDstopwordsANDstemmingANDtfidf(self, texts):\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words=self.stopwords, use_idf=True, smooth_idf=True, tokenizer=StemTokenizer(self.stopwords))\n",
    "        x_train_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        dense_tfidf = x_train_tfidf.todense()\n",
    "        tfidf_df = pd.DataFrame(dense_tfidf, columns=feature_names)\n",
    "        important_tokens = tfidf_df.columns[(tfidf_df > self.threshold).any(axis=0)]  # Corrected from self.stopwords\n",
    "\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            binary=True,\n",
    "            tokenizer=StemTokenizer(self.stopwords),\n",
    "            stop_words=self.stopwords,\n",
    "            vocabulary=important_tokens\n",
    "        )\n",
    "        x_train = self.vectorizer.fit_transform(texts)\n",
    "        n_samples = x_train.shape[0]  # Number of samples\n",
    "        print(f\"Binary vectorized training dataset (WITH stop words consideration and STEMMING and TFIDF based token selection [thresold = {self.threshold}]) has {n_samples} examples and {x_train.shape[1]} features\")\n",
    "        return x_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized training dataset has 1399 examples and 13690 features\n",
      "Binary vectorized training dataset (WITH stop words consideration) has 1399 examples and 13461 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized training dataset (WITH stop words consideration and STEMMING) has 1399 examples and 8635 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/clatimie/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized training dataset (WITH stop words consideration and STEMMING and TFIDF based token selection [thresold = 0.2]) has 1399 examples and 5231 features\n",
      "Binary vectorized test dataset (WITH stop words consideration and STEMMING and TFIDF based token selection [thresold = 0.2]) has 600 examples and 5231 features\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 16098 stored elements and shape (600, 5231)>\n",
      "  Coords\tValues\n",
      "  (0, 126)\t1\n",
      "  (0, 666)\t1\n",
      "  (0, 1058)\t1\n",
      "  (0, 1468)\t1\n",
      "  (0, 1593)\t1\n",
      "  (0, 1839)\t1\n",
      "  (0, 1946)\t1\n",
      "  (0, 2024)\t1\n",
      "  (0, 2214)\t1\n",
      "  (0, 2227)\t1\n",
      "  (0, 2540)\t1\n",
      "  (0, 2689)\t1\n",
      "  (0, 2790)\t1\n",
      "  (0, 3064)\t1\n",
      "  (0, 3181)\t1\n",
      "  (0, 3248)\t1\n",
      "  (0, 3288)\t1\n",
      "  (0, 3382)\t1\n",
      "  (0, 3560)\t1\n",
      "  (0, 3616)\t1\n",
      "  (0, 3820)\t1\n",
      "  (0, 3922)\t1\n",
      "  (0, 4029)\t1\n",
      "  (0, 4317)\t1\n",
      "  (0, 4384)\t1\n",
      "  :\t:\n",
      "  (599, 2059)\t1\n",
      "  (599, 2449)\t1\n",
      "  (599, 2687)\t1\n",
      "  (599, 2924)\t1\n",
      "  (599, 3201)\t1\n",
      "  (599, 3219)\t1\n",
      "  (599, 3230)\t1\n",
      "  (599, 3382)\t1\n",
      "  (599, 3461)\t1\n",
      "  (599, 3797)\t1\n",
      "  (599, 3959)\t1\n",
      "  (599, 4005)\t1\n",
      "  (599, 4279)\t1\n",
      "  (599, 4290)\t1\n",
      "  (599, 4347)\t1\n",
      "  (599, 4494)\t1\n",
      "  (599, 4692)\t1\n",
      "  (599, 4713)\t1\n",
      "  (599, 4736)\t1\n",
      "  (599, 4746)\t1\n",
      "  (599, 5034)\t1\n",
      "  (599, 5054)\t1\n",
      "  (599, 5075)\t1\n",
      "  (599, 5109)\t1\n",
      "  (599, 5183)\t1\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords list\n",
    "stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "\n",
    "# Approach 1: Binary representation of words\n",
    "BinaryVectorizer = Vectorizer(method=\"binary\", isTraining=True)\n",
    "x_train_1 = BinaryVectorizer.vectorize(texts_train)  # Use the instance to call vectorize\n",
    "BinaryVectorizer.change_mode() # test mode \n",
    "\n",
    "# Approach 2: Binary representation of words with stop words considered\n",
    "BinaryStopWordsVectorizer = Vectorizer(method=\"binaryANDstopwords\", isTraining=True, stopwords_list=stopwords_list)\n",
    "x_train_2 = BinaryStopWordsVectorizer.vectorize(texts_train)  # Use the instance\n",
    "BinaryStopWordsVectorizer.change_mode() # test mode \n",
    "\n",
    "\n",
    "# Approach 3: Binary representation of words with stop words considered and stemming\n",
    "BinaryStopWordsStemmingVectorizer = Vectorizer(method=\"binaryANDstopwordsANDstemming\", isTraining=True,stopwords_list=stopwords_list)\n",
    "x_train_3 = BinaryStopWordsStemmingVectorizer.vectorize(texts_train)  # Use the instance\n",
    "BinaryStopWordsStemmingVectorizer.change_mode() # test mode \n",
    "\n",
    "# Approach 4: Binary representation with stop words, stemming, and TF-IDF selection -> hyperparameter need to be tuned  \n",
    "BinaryStopWordsStemmingTFIDFVectorizer = Vectorizer(method=\"binaryANDstopwordsANDstemmingANDtfidf\", isTraining=True, stopwords_list=stopwords_list, threshold=0.2)\n",
    "x_train_4 = BinaryStopWordsStemmingTFIDFVectorizer.vectorize(texts_train)  # Use the instance\n",
    "BinaryStopWordsStemmingTFIDFVectorizer.change_mode() # test mode \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
