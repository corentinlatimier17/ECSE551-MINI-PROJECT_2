{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Subreddit prediction</u> #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description of the project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Project overview </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project aims to develop machine learning models for **analyzing Reddit text** to determine the origin subreddit of a given post or comment. Reddit, a popular social media platform, is organized into a variety of thematic communities known as *subreddits*, where users share content and engage in discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color: #FF9800;\">Objective </span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective is to build a model that can **predict the subreddit** of a Reddit post or comment. Given a text entry from Reddit, the model will identify which of the following subreddits it originally came from:\n",
    "\n",
    "- **Toronto**\n",
    "- **Brussels**\n",
    "- **London**\n",
    "- **Montreal**\n",
    "\n",
    "<b>This defines a multiclass classification problem</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Approach</span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project consists of two main parts:\n",
    "\n",
    "1. **Implement a Bernoulli Naïve Bayes Classifier from Scratch**  \n",
    "   First, a Bernoulli Naïve Bayes classifier will be developed from the ground up, without relying on external libraries for the core algorithm. This implementation will provide a deeper understanding of how the Bernoulli Naïve Bayes method works and how it can be applied to text classification.\n",
    "\n",
    "2. **Utilize a Classifier from Scikit-Learn**  \n",
    "   In the second part, a pre-built classifier from the `scikit-learn` library will be used to perform the same task. This comparison will allow us to evaluate the effectiveness of our custom implementation against a widely used, optimized machine learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset and modules ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Module importation </span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Load training dataset</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 1399 examples and 4 classes\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_training = \"../datasets/Train.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "training_data = pd.read_csv(path_training, delimiter=',')\n",
    "\n",
    "# Set column names explicitly for better readability\n",
    "training_data.columns = ['text', 'subreddit']\n",
    "\n",
    "# Separate the training data into two series: texts and subreddit labels\n",
    "texts_train = training_data['text']          # Contains the Reddit posts or comments\n",
    "subreddits_train = training_data['subreddit'] # Contains the subreddit each post originates from\n",
    "\n",
    "# Get unique subreddit labels\n",
    "unique_labels = np.unique(subreddits_train)   # List of unique subreddits in the dataset\n",
    "\n",
    "n_samples = texts_train.shape[0]\n",
    "n_classes = unique_labels.shape[0]\n",
    "\n",
    "print(f\"Dataset has {n_samples} examples and {n_classes} classes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
