{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Subreddit prediction</u> #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description of the project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Project overview </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project aims to develop machine learning models for **analyzing Reddit text** to determine the origin subreddit of a given post or comment. Reddit, a popular social media platform, is organized into a variety of thematic communities known as *subreddits*, where users share content and engage in discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color: #FF9800;\">Objective </span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective is to build a model that can **predict the subreddit** of a Reddit post or comment. Given a text entry from Reddit, the model will identify which of the following subreddits it originally came from:\n",
    "\n",
    "- **Toronto**\n",
    "- **Brussels**\n",
    "- **London**\n",
    "- **Montreal**\n",
    "\n",
    "<b>This defines a multiclass classification problem</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Approach</span> ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project consists of two main parts:\n",
    "\n",
    "1. **Implement a Bernoulli Naïve Bayes Classifier from Scratch**  \n",
    "   First, a Bernoulli Naïve Bayes classifier will be developed from the ground up, without relying on external libraries for the core algorithm. This implementation will provide a deeper understanding of how the Bernoulli Naïve Bayes method works and how it can be applied to text classification.\n",
    "\n",
    "2. **Utilize a Classifier from Scikit-Learn**  \n",
    "   In the second part, a pre-built classifier from the `scikit-learn` library will be used to perform the same task. This comparison will allow us to evaluate the effectiveness of our custom implementation against a widely used, optimized machine learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset and modules ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Module importation </span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #FF9800;\">Load training dataset</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 1399 examples and 4 classes\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the training data file\n",
    "path_training = \"../datasets/Train.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "training_data = pd.read_csv(path_training, delimiter=',')\n",
    "\n",
    "# Set column names explicitly for better readability\n",
    "training_data.columns = ['text', 'subreddit']\n",
    "\n",
    "# Separate the training data into two series: texts and subreddit labels\n",
    "texts_train = training_data['text']          # Contains the Reddit posts or comments\n",
    "subreddits_train = training_data['subreddit'] # Contains the subreddit each post originates from\n",
    "\n",
    "# Get unique subreddit labels\n",
    "unique_labels = np.unique(subreddits_train)   # List of unique subreddits in the dataset\n",
    "\n",
    "n_samples = texts_train.shape[0]\n",
    "n_classes = unique_labels.shape[0]\n",
    "\n",
    "print(f\"Dataset has {n_samples} examples and {n_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization of the texts ##\n",
    "\n",
    "To utilize the texts in machine learning models, it is essential to convert them into a vectorized format. There are several methods available for encoding texts as vectors.\n",
    "\n",
    "1. **Binary Representation of Words**  \n",
    "   One approach is to employ a binary representation of the words. This method indicates the presence or absence of each word in the text using binary values (1 or 0).\n",
    "\n",
    "2. **Removal of Stop Words**  \n",
    "   Additionally, it is important to consider the removal of stop words—common words such as \"and,\" \"the,\" or \"is\" that may not carry significant meaning in the context of the analysis. By eliminating these words, we can enhance the quality of our feature set.\n",
    "\n",
    "3. **TF-IDF (Term Frequency-Inverse Document Frequency)**  \n",
    "   Another effective technique for vectorization is the use of TF-IDF. This method not only accounts for the frequency of words in the text but also adjusts for their importance across the entire corpus. By selecting features based on TF-IDF scores, we can focus on the most relevant words for our machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary vectorized dataset(WITHOUT stop words consideration) has 1399 and 13690 features\n",
      "Binary vectorized dataset(WITH stop words consideration) has 1399 and 13405 features\n"
     ]
    }
   ],
   "source": [
    "# Approach 1 :  Binary representation of words (present [1] or absent [0]) -> no stop words considered and no words selection\n",
    "BinaryVectorizer = CountVectorizer(binary=True)\n",
    "x_train_1 = BinaryVectorizer.fit_transform(texts_train)\n",
    "n_features_1 = x_train_1.shape[1]\n",
    "print(f\"Binary vectorized dataset(WITHOUT stop words consideration) has {n_samples} and {n_features_1} features\")\n",
    "\n",
    "# Approach 2 : Binary representation of words (present [1] or absent [0]) with stop words considered\n",
    "BinaryVectorizerStopWords = CountVectorizer(binary=True, stop_words=list(ENGLISH_STOP_WORDS))\n",
    "x_train_2 = BinaryVectorizerStopWords.fit_transform(texts_train)\n",
    "n_features_2 = x_train_2.shape[1]\n",
    "print(f\"Binary vectorized dataset(WITH stop words consideration) has {n_samples} and {n_features_2} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
